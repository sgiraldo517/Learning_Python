{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d56b50c-fcbc-41ea-a120-5d61905107fd",
   "metadata": {},
   "source": [
    "# Data Manipulation with pandas\n",
    "----\n",
    "## Transforming DataFrames\n",
    "----\n",
    "\n",
    "### Introducing Dataframes\n",
    "**Pandas** is built on Numpy and Matplotlib. It is used to work with rectangular (tabular) data that in pandas are called Dataframes. Every value within a column has the same data type but differnt columns can have different data types.\n",
    "\n",
    "To quickly explore a dataframe, pandas has several methods: \n",
    "1. **.head():** returns the first few rows in a Dataframe\n",
    "2. **.info():** displays the names of the columns, the data type and if they have any missing values.  \n",
    "3. **.shape** will display the rows and colums as **(#rows, #colums)**. **Note:** *this is an attribute so we do not write the parentheses*\n",
    "4. **.describe():** compute some summary statistics for numerical columns \n",
    "\n",
    "Dataframes  consist of three components, stored as attributes:\n",
    "1. **.values:** A two-dimensional NumPy array of values.\n",
    "2. **.columns:** An index of columns: the column names.\n",
    "3. **.index:** An index for the rows: either row numbers or row names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec53575-9376-450f-af12-3e9fb36a27a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0              region       state  individuals  family_members  \\\n",
      "0           0  East South Central     Alabama       2570.0           864.0   \n",
      "1           1             Pacific      Alaska       1434.0           582.0   \n",
      "2           2            Mountain     Arizona       7259.0          2606.0   \n",
      "3           3  West South Central    Arkansas       2280.0           432.0   \n",
      "4           4             Pacific  California     109008.0         20964.0   \n",
      "\n",
      "   state_pop  \n",
      "0    4887681  \n",
      "1     735139  \n",
      "2    7158024  \n",
      "3    3009733  \n",
      "4   39461588  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 51 entries, 0 to 50\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Unnamed: 0      51 non-null     int64  \n",
      " 1   region          51 non-null     object \n",
      " 2   state           51 non-null     object \n",
      " 3   individuals     51 non-null     float64\n",
      " 4   family_members  51 non-null     float64\n",
      " 5   state_pop       51 non-null     int64  \n",
      "dtypes: float64(2), int64(2), object(2)\n",
      "memory usage: 2.5+ KB\n",
      "None\n",
      "(51, 6)\n",
      "       Unnamed: 0    individuals  family_members     state_pop\n",
      "count   51.000000      51.000000       51.000000  5.100000e+01\n",
      "mean    25.000000    7225.784314     3504.882353  6.405637e+06\n",
      "std     14.866069   15991.025083     7805.411811  7.327258e+06\n",
      "min      0.000000     434.000000       75.000000  5.776010e+05\n",
      "25%     12.500000    1446.500000      592.000000  1.777414e+06\n",
      "50%     25.000000    3082.000000     1482.000000  4.461153e+06\n",
      "75%     37.500000    6781.500000     3196.000000  7.340946e+06\n",
      "max     50.000000  109008.000000    52070.000000  3.946159e+07\n",
      "[[0 'East South Central' 'Alabama' 2570.0 864.0 4887681]\n",
      " [1 'Pacific' 'Alaska' 1434.0 582.0 735139]\n",
      " [2 'Mountain' 'Arizona' 7259.0 2606.0 7158024]\n",
      " [3 'West South Central' 'Arkansas' 2280.0 432.0 3009733]\n",
      " [4 'Pacific' 'California' 109008.0 20964.0 39461588]\n",
      " [5 'Mountain' 'Colorado' 7607.0 3250.0 5691287]\n",
      " [6 'New England' 'Connecticut' 2280.0 1696.0 3571520]\n",
      " [7 'South Atlantic' 'Delaware' 708.0 374.0 965479]\n",
      " [8 'South Atlantic' 'District of Columbia' 3770.0 3134.0 701547]\n",
      " [9 'South Atlantic' 'Florida' 21443.0 9587.0 21244317]\n",
      " [10 'South Atlantic' 'Georgia' 6943.0 2556.0 10511131]\n",
      " [11 'Pacific' 'Hawaii' 4131.0 2399.0 1420593]\n",
      " [12 'Mountain' 'Idaho' 1297.0 715.0 1750536]\n",
      " [13 'East North Central' 'Illinois' 6752.0 3891.0 12723071]\n",
      " [14 'East North Central' 'Indiana' 3776.0 1482.0 6695497]\n",
      " [15 'West North Central' 'Iowa' 1711.0 1038.0 3148618]\n",
      " [16 'West North Central' 'Kansas' 1443.0 773.0 2911359]\n",
      " [17 'East South Central' 'Kentucky' 2735.0 953.0 4461153]\n",
      " [18 'West South Central' 'Louisiana' 2540.0 519.0 4659690]\n",
      " [19 'New England' 'Maine' 1450.0 1066.0 1339057]\n",
      " [20 'South Atlantic' 'Maryland' 4914.0 2230.0 6035802]\n",
      " [21 'New England' 'Massachusetts' 6811.0 13257.0 6882635]\n",
      " [22 'East North Central' 'Michigan' 5209.0 3142.0 9984072]\n",
      " [23 'West North Central' 'Minnesota' 3993.0 3250.0 5606249]\n",
      " [24 'East South Central' 'Mississippi' 1024.0 328.0 2981020]\n",
      " [25 'West North Central' 'Missouri' 3776.0 2107.0 6121623]\n",
      " [26 'Mountain' 'Montana' 983.0 422.0 1060665]\n",
      " [27 'West North Central' 'Nebraska' 1745.0 676.0 1925614]\n",
      " [28 'Mountain' 'Nevada' 7058.0 486.0 3027341]\n",
      " [29 'New England' 'New Hampshire' 835.0 615.0 1353465]\n",
      " [30 'Mid-Atlantic' 'New Jersey' 6048.0 3350.0 8886025]\n",
      " [31 'Mountain' 'New Mexico' 1949.0 602.0 2092741]\n",
      " [32 'Mid-Atlantic' 'New York' 39827.0 52070.0 19530351]\n",
      " [33 'South Atlantic' 'North Carolina' 6451.0 2817.0 10381615]\n",
      " [34 'West North Central' 'North Dakota' 467.0 75.0 758080]\n",
      " [35 'East North Central' 'Ohio' 6929.0 3320.0 11676341]\n",
      " [36 'West South Central' 'Oklahoma' 2823.0 1048.0 3940235]\n",
      " [37 'Pacific' 'Oregon' 11139.0 3337.0 4181886]\n",
      " [38 'Mid-Atlantic' 'Pennsylvania' 8163.0 5349.0 12800922]\n",
      " [39 'New England' 'Rhode Island' 747.0 354.0 1058287]\n",
      " [40 'South Atlantic' 'South Carolina' 3082.0 851.0 5084156]\n",
      " [41 'West North Central' 'South Dakota' 836.0 323.0 878698]\n",
      " [42 'East South Central' 'Tennessee' 6139.0 1744.0 6771631]\n",
      " [43 'West South Central' 'Texas' 19199.0 6111.0 28628666]\n",
      " [44 'Mountain' 'Utah' 1904.0 972.0 3153550]\n",
      " [45 'New England' 'Vermont' 780.0 511.0 624358]\n",
      " [46 'South Atlantic' 'Virginia' 3928.0 2047.0 8501286]\n",
      " [47 'Pacific' 'Washington' 16424.0 5880.0 7523869]\n",
      " [48 'South Atlantic' 'West Virginia' 1021.0 222.0 1804291]\n",
      " [49 'East North Central' 'Wisconsin' 2740.0 2167.0 5807406]\n",
      " [50 'Mountain' 'Wyoming' 434.0 205.0 577601]]\n",
      "Index(['Unnamed: 0', 'region', 'state', 'individuals', 'family_members',\n",
      "       'state_pop'],\n",
      "      dtype='object')\n",
      "RangeIndex(start=0, stop=51, step=1)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "homelessness = pd.read_csv(\"Datasets\\homelessness.csv\")\n",
    "\n",
    "# Print the head of the homelessness data\n",
    "print(homelessness.head())\n",
    "\n",
    "# Print information about homelessness\n",
    "print(homelessness.info())\n",
    "\n",
    "# Print the shape of homelessness\n",
    "print(homelessness.shape)\n",
    "\n",
    "# Print a description of homelessness\n",
    "print(homelessness.describe())\n",
    "\n",
    "# Print the values of homelessness\n",
    "print(homelessness.values)\n",
    "\n",
    "# Print the column index of homelessness\n",
    "print(homelessness.columns)\n",
    "\n",
    "# Print the row index of homelessness\n",
    "print(homelessness.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde8fc7",
   "metadata": {},
   "source": [
    "### Sorting and subsetting\n",
    "\n",
    "#### Sorting \n",
    "Change the order of the rows `.sort_values(\"column_name_to_sort_by\")`. \n",
    "- Setting the argument ascending=False will sort the data descending. \n",
    "- It is possible to sort by different columns `.sort_values([\"first_column_name_to_sort_by\", \"second_column_name_to_sort_by\"])`. The dataframe will be sorted by the first stated column and then the second, ... \n",
    "\n",
    "#### Subsetting \n",
    "To zoom in over just one column we will use `dataframe_name[\"column_name\"]`. \n",
    "- To select multuple columns we need to use 2 sets of brackets: `dataframe_name[[\"first_column_name\", \"second_column_name\"]]`. \n",
    "  - The out square brackets are responsible for subsetting the dataframe `dataframe[\"cols_to_subset\"]`\n",
    "  - the in square brackets are creating a list of columns names to subset `cols_to_subset[\"column1\", \"column2]`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e771f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0              region         state  individuals  family_members  \\\n",
      "50          50            Mountain       Wyoming        434.0           205.0   \n",
      "34          34  West North Central  North Dakota        467.0            75.0   \n",
      "7            7      South Atlantic      Delaware        708.0           374.0   \n",
      "39          39         New England  Rhode Island        747.0           354.0   \n",
      "45          45         New England       Vermont        780.0           511.0   \n",
      "\n",
      "    state_pop  \n",
      "50     577601  \n",
      "34     758080  \n",
      "7      965479  \n",
      "39    1058287  \n",
      "45     624358  \n",
      "    Unnamed: 0              region          state  individuals  \\\n",
      "32          32        Mid-Atlantic       New York      39827.0   \n",
      "4            4             Pacific     California     109008.0   \n",
      "21          21         New England  Massachusetts       6811.0   \n",
      "9            9      South Atlantic        Florida      21443.0   \n",
      "43          43  West South Central          Texas      19199.0   \n",
      "\n",
      "    family_members  state_pop  \n",
      "32         52070.0   19530351  \n",
      "4          20964.0   39461588  \n",
      "21         13257.0    6882635  \n",
      "9           9587.0   21244317  \n",
      "43          6111.0   28628666  \n",
      "    Unnamed: 0              region      state  individuals  family_members  \\\n",
      "13          13  East North Central   Illinois       6752.0          3891.0   \n",
      "35          35  East North Central       Ohio       6929.0          3320.0   \n",
      "22          22  East North Central   Michigan       5209.0          3142.0   \n",
      "49          49  East North Central  Wisconsin       2740.0          2167.0   \n",
      "14          14  East North Central    Indiana       3776.0          1482.0   \n",
      "\n",
      "    state_pop  \n",
      "13   12723071  \n",
      "35   11676341  \n",
      "22    9984072  \n",
      "49    5807406  \n",
      "14    6695497  \n"
     ]
    }
   ],
   "source": [
    "# Sort homelessness by individuals\n",
    "homelessness_ind = homelessness.sort_values(\"individuals\")\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_ind.head())\n",
    "\n",
    "# Sort homelessness by descending family members\n",
    "homelessness_fam = homelessness.sort_values(\"family_members\", ascending=False)\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_fam.head())\n",
    "\n",
    "# Sort homelessness by region, then descending family members\n",
    "homelessness_reg_fam = homelessness.sort_values([\"region\", \"family_members\"], ascending = [True,False])\n",
    "\n",
    "# Print the top few rows\n",
    "print(homelessness_reg_fam.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee38ba99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2570.0\n",
      "1      1434.0\n",
      "2      7259.0\n",
      "3      2280.0\n",
      "4    109008.0\n",
      "Name: individuals, dtype: float64\n",
      "        state  family_members\n",
      "0     Alabama           864.0\n",
      "1      Alaska           582.0\n",
      "2     Arizona          2606.0\n",
      "3    Arkansas           432.0\n",
      "4  California         20964.0\n",
      "   individuals       state\n",
      "0       2570.0     Alabama\n",
      "1       1434.0      Alaska\n",
      "2       7259.0     Arizona\n",
      "3       2280.0    Arkansas\n",
      "4     109008.0  California\n"
     ]
    }
   ],
   "source": [
    "# Select the individuals column\n",
    "individuals = homelessness[\"individuals\"]\n",
    "\n",
    "# Print the head of the result\n",
    "print(individuals.head())\n",
    "\n",
    "# Select the state and family_members columns\n",
    "state_fam = homelessness[[\"state\", \"family_members\"]]\n",
    "\n",
    "# Print the head of the result\n",
    "print(state_fam.head())\n",
    "\n",
    "# Select only the individuals and state columns, in that order\n",
    "ind_state = homelessness[[\"individuals\", \"state\"]]\n",
    "\n",
    "# Print the head of the result\n",
    "print(ind_state.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "282ea007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0              region       state  individuals  family_members  \\\n",
      "4            4             Pacific  California     109008.0         20964.0   \n",
      "9            9      South Atlantic     Florida      21443.0          9587.0   \n",
      "32          32        Mid-Atlantic    New York      39827.0         52070.0   \n",
      "37          37             Pacific      Oregon      11139.0          3337.0   \n",
      "43          43  West South Central       Texas      19199.0          6111.0   \n",
      "47          47             Pacific  Washington      16424.0          5880.0   \n",
      "\n",
      "    state_pop  \n",
      "4    39461588  \n",
      "9    21244317  \n",
      "32   19530351  \n",
      "37    4181886  \n",
      "43   28628666  \n",
      "47    7523869  \n"
     ]
    }
   ],
   "source": [
    "# Filter for rows where individuals is greater than 10000\n",
    "ind_gt_10k = homelessness[homelessness[\"individuals\"] > 10000]\n",
    "\n",
    "# See the result\n",
    "print(ind_gt_10k)\n",
    "\n",
    "# Filter for rows where region is Mountain\n",
    "mountain_reg = homelessness[homelessness[\"region\"] == 'Mountain']\n",
    "\n",
    "# See the result\n",
    "print(mountain_reg)\n",
    "\n",
    "# Filter for rows where family_members is less than 1000 and region is Pacific\n",
    "fam_lt_1k_pac = homelessness[(homelessness[\"family_members\"] < 1000) & (homelessness[\"region\"] == \"Pacific\")]\n",
    "\n",
    "# See the result\n",
    "print(fam_lt_1k_pac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc467de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0          region                 state  individuals  \\\n",
      "7            7  South Atlantic              Delaware        708.0   \n",
      "8            8  South Atlantic  District of Columbia       3770.0   \n",
      "9            9  South Atlantic               Florida      21443.0   \n",
      "10          10  South Atlantic               Georgia       6943.0   \n",
      "20          20  South Atlantic              Maryland       4914.0   \n",
      "30          30    Mid-Atlantic            New Jersey       6048.0   \n",
      "32          32    Mid-Atlantic              New York      39827.0   \n",
      "33          33  South Atlantic        North Carolina       6451.0   \n",
      "38          38    Mid-Atlantic          Pennsylvania       8163.0   \n",
      "40          40  South Atlantic        South Carolina       3082.0   \n",
      "46          46  South Atlantic              Virginia       3928.0   \n",
      "48          48  South Atlantic         West Virginia       1021.0   \n",
      "\n",
      "    family_members  state_pop  \n",
      "7            374.0     965479  \n",
      "8           3134.0     701547  \n",
      "9           9587.0   21244317  \n",
      "10          2556.0   10511131  \n",
      "20          2230.0    6035802  \n",
      "30          3350.0    8886025  \n",
      "32         52070.0   19530351  \n",
      "33          2817.0   10381615  \n",
      "38          5349.0   12800922  \n",
      "40           851.0    5084156  \n",
      "46          2047.0    8501286  \n",
      "48           222.0    1804291  \n",
      "    Unnamed: 0          region                 state  individuals  \\\n",
      "7            7  South Atlantic              Delaware        708.0   \n",
      "8            8  South Atlantic  District of Columbia       3770.0   \n",
      "9            9  South Atlantic               Florida      21443.0   \n",
      "10          10  South Atlantic               Georgia       6943.0   \n",
      "20          20  South Atlantic              Maryland       4914.0   \n",
      "30          30    Mid-Atlantic            New Jersey       6048.0   \n",
      "32          32    Mid-Atlantic              New York      39827.0   \n",
      "33          33  South Atlantic        North Carolina       6451.0   \n",
      "38          38    Mid-Atlantic          Pennsylvania       8163.0   \n",
      "40          40  South Atlantic        South Carolina       3082.0   \n",
      "46          46  South Atlantic              Virginia       3928.0   \n",
      "48          48  South Atlantic         West Virginia       1021.0   \n",
      "\n",
      "    family_members  state_pop  \n",
      "7            374.0     965479  \n",
      "8           3134.0     701547  \n",
      "9           9587.0   21244317  \n",
      "10          2556.0   10511131  \n",
      "20          2230.0    6035802  \n",
      "30          3350.0    8886025  \n",
      "32         52070.0   19530351  \n",
      "33          2817.0   10381615  \n",
      "38          5349.0   12800922  \n",
      "40           851.0    5084156  \n",
      "46          2047.0    8501286  \n",
      "48           222.0    1804291  \n"
     ]
    }
   ],
   "source": [
    "# Subsetting rows by categorical variables\n",
    "\n",
    "# METHOD 1\n",
    "# Subset for rows in South Atlantic or Mid-Atlantic regions\n",
    "south_mid_atlantic_method_1 = homelessness[(homelessness[\"region\"] == \"South Atlantic\") | (homelessness[\"region\"] == \"Mid-Atlantic\")]\n",
    "\n",
    "# See the result\n",
    "print(south_mid_atlantic_method_1)\n",
    "\n",
    "# METHOD 2\n",
    "south_mid_atlantic_method_2 = [\"South Atlantic\", \"Mid-Atlantic\"]\n",
    "condition = homelessness[\"region\"].isin(south_mid_atlantic_method_2)\n",
    "print(homelessness[condition])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0a7ab32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0    region       state  individuals  family_members  state_pop\n",
      "2            2  Mountain     Arizona       7259.0          2606.0    7158024\n",
      "4            4   Pacific  California     109008.0         20964.0   39461588\n",
      "28          28  Mountain      Nevada       7058.0           486.0    3027341\n",
      "44          44  Mountain        Utah       1904.0           972.0    3153550\n"
     ]
    }
   ],
   "source": [
    "# The Mojave Desert states\n",
    "canu = [\"California\", \"Arizona\", \"Nevada\", \"Utah\"]\n",
    "\n",
    "# Filter for rows in the Mojave Desert states\n",
    "mojave_homelessness = homelessness[homelessness[\"state\"].isin(canu)]\n",
    "\n",
    "# See the result\n",
    "print(mojave_homelessness)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8702b27",
   "metadata": {},
   "source": [
    "### New Colums\n",
    "You can add new columns to a DataFrame. This has many names, such as transforming, mutating, and feature engineering.Also, you can create new columns from scratch, but it is also common to derive them from other columns, for example, by adding columns together or by changing their units.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a80b574a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0              region       state  individuals  family_members  \\\n",
      "0           0  East South Central     Alabama       2570.0           864.0   \n",
      "1           1             Pacific      Alaska       1434.0           582.0   \n",
      "2           2            Mountain     Arizona       7259.0          2606.0   \n",
      "3           3  West South Central    Arkansas       2280.0           432.0   \n",
      "4           4             Pacific  California     109008.0         20964.0   \n",
      "\n",
      "   state_pop     total  p_individuals  \n",
      "0    4887681    3434.0       0.748398  \n",
      "1     735139    2016.0       0.711310  \n",
      "2    7158024    9865.0       0.735834  \n",
      "3    3009733    2712.0       0.840708  \n",
      "4   39461588  129972.0       0.838704  \n"
     ]
    }
   ],
   "source": [
    "# Add total col as sum of individuals and family_members\n",
    "homelessness[\"total\"] = homelessness[\"individuals\"] + homelessness[\"family_members\"]\n",
    "\n",
    "# Add p_individuals col as proportion of total that are individuals\n",
    "homelessness[\"p_individuals\"] = homelessness[\"individuals\"] / homelessness[\"total\"]\n",
    "\n",
    "# See the result\n",
    "print(homelessness.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfcd1399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   state  indiv_per_10k\n",
      "8   District of Columbia      53.738381\n",
      "11                Hawaii      29.079406\n",
      "4             California      27.623825\n",
      "37                Oregon      26.636307\n",
      "28                Nevada      23.314189\n",
      "47            Washington      21.829195\n",
      "32              New York      20.392363\n"
     ]
    }
   ],
   "source": [
    "# Which state has the highest number of homeless individuals per 10,000 people in the state?\n",
    "\n",
    "# Create indiv_per_10k col as homeless individuals per 10k state pop\n",
    "homelessness[\"indiv_per_10k\"] = 10000 * homelessness[\"individuals\"] / homelessness[\"state_pop\"] \n",
    "\n",
    "# Subset rows for indiv_per_10k greater than 20\n",
    "high_homelessness = homelessness[homelessness[\"indiv_per_10k\"] > 20]\n",
    "\n",
    "# Sort high_homelessness by descending indiv_per_10k\n",
    "high_homelessness_srt = high_homelessness.sort_values(\"indiv_per_10k\", ascending=False)\n",
    "\n",
    "# From high_homelessness_srt, select the state and indiv_per_10k cols\n",
    "result = high_homelessness_srt[[\"state\", \"indiv_per_10k\"]]\n",
    "\n",
    "# See the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating Dataframes\n",
    "----\n",
    "### Summary statistics\n",
    "Numbers that summarize and tell you about your dataset. For example, mean, median, minimum, maximum, and standard deviation. To use them only select the column of the dataframe using [] and use one of the following methods: \n",
    "- .mean()\n",
    "- .median()\n",
    "- .mode()\n",
    "- .min()\n",
    "- .max()\n",
    "- .var()\n",
    "- .std()\n",
    "- .sum()\n",
    "- .quantile()\n",
    "\n",
    "Example: `dataframe[\"column_name\"].mean()`\n",
    "\n",
    ">**Note:** you can apply this methods to numeric and date columns (The date type should be datetime64). \n",
    "\n",
    "**Additional usefull methods**\n",
    "- .agg()\n",
    "- .cumsum(): Calculates the cumulative sum of the rows. \n",
    "- .cummax()\n",
    "- .cummin()\n",
    "- .cumprod()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0           0      1    A           1  2010-02-05      24924.50       False   \n",
      "1           1      1    A           1  2010-03-05      21827.90       False   \n",
      "2           2      1    A           1  2010-04-02      57258.43       False   \n",
      "3           3      1    A           1  2010-05-07      17413.94       False   \n",
      "4           4      1    A           1  2010-06-04      17558.09       False   \n",
      "\n",
      "   temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0       5.727778              0.679451         8.106  \n",
      "1       8.055556              0.693452         8.106  \n",
      "2      16.816667              0.718284         7.808  \n",
      "3      22.527778              0.748928         7.808  \n",
      "4      27.050000              0.714586         7.808  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10774 entries, 0 to 10773\n",
      "Data columns (total 10 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Unnamed: 0            10774 non-null  int64  \n",
      " 1   store                 10774 non-null  int64  \n",
      " 2   type                  10774 non-null  object \n",
      " 3   department            10774 non-null  int64  \n",
      " 4   date                  10774 non-null  object \n",
      " 5   weekly_sales          10774 non-null  float64\n",
      " 6   is_holiday            10774 non-null  bool   \n",
      " 7   temperature_c         10774 non-null  float64\n",
      " 8   fuel_price_usd_per_l  10774 non-null  float64\n",
      " 9   unemployment          10774 non-null  float64\n",
      "dtypes: bool(1), float64(4), int64(3), object(2)\n",
      "memory usage: 768.2+ KB\n",
      "None\n",
      "23843.95014850566\n",
      "12049.064999999999\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sales = pd.read_csv(\"Datasets\\sales_subset.csv\")\n",
    "\n",
    "# Print the head of the sales DataFrame\n",
    "print(sales.head())\n",
    "\n",
    "# Print the info about the sales DataFrame\n",
    "print(sales.info())\n",
    "\n",
    "# Print the mean of weekly_sales\n",
    "print(sales[\"weekly_sales\"].mean())\n",
    "\n",
    "# Print the median of weekly_sales\n",
    "print(sales[\"weekly_sales\"].median())\n",
    "\n",
    "# Print the maximum of the date column\n",
    "print(sales[\"date\"].max())\n",
    "\n",
    "# Print the minimum of the date column\n",
    "print(sales[\"date\"].min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The .agg() method: Efficient summaries\n",
    "\n",
    "The .agg() method allows you to apply your own custom functions to a DataFrame, as well as apply functions to more than one column of a DataFrame at once, making your aggregations super-efficient. \n",
    "\n",
    "`df['column'].agg(function)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NumPy and create custom IQR function\n",
    "import numpy as np\n",
    "def iqr(column):\n",
    "    return column.quantile(0.75) - column.quantile(0.25)\n",
    "\n",
    "# Update to print IQR and median of temperature_c, fuel_price_usd_per_l, & unemployment\n",
    "print(sales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.median]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting: Summarizing categorical data\n",
    "1. Droping duplicated values in only 1 column or multiple columns: `dataframe.drop_duplicates(subset=\"column_name\")` or `dataframe.drop_duplicates(subset=[\"column_name_1\", \"column_name_2\")`\n",
    "2. Counting the values in a column: `dataframe[\"column_name\"].value_counts()`. \n",
    "   1. The sort parameter can be added to order the data based on the count result `dataframe[\"column_name\"].value_counts(sort=True)`\n",
    "   2. The normalize parameter can be used to get the results as a proportion of the total `dataframe[\"column_name\"].value_counts(normalize=True)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0  store type  department        date  weekly_sales  \\\n",
      "0              0      1    A           1  2010-02-05      24924.50   \n",
      "901          901      2    A           1  2010-02-05      35034.06   \n",
      "1798        1798      4    A           1  2010-02-05      38724.42   \n",
      "2699        2699      6    A           1  2010-02-05      25619.00   \n",
      "3593        3593     10    B           1  2010-02-05      40212.84   \n",
      "\n",
      "      is_holiday  temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0          False       5.727778              0.679451         8.106  \n",
      "901        False       4.550000              0.679451         8.324  \n",
      "1798       False       6.533333              0.686319         8.623  \n",
      "2699       False       4.683333              0.679451         7.259  \n",
      "3593       False      12.411111              0.782478         9.765  \n",
      "    Unnamed: 0  store type  department        date  weekly_sales  is_holiday  \\\n",
      "0            0      1    A           1  2010-02-05      24924.50       False   \n",
      "12          12      1    A           2  2010-02-05      50605.27       False   \n",
      "24          24      1    A           3  2010-02-05      13740.12       False   \n",
      "36          36      1    A           4  2010-02-05      39954.04       False   \n",
      "48          48      1    A           5  2010-02-05      32229.38       False   \n",
      "\n",
      "    temperature_c  fuel_price_usd_per_l  unemployment  \n",
      "0        5.727778              0.679451         8.106  \n",
      "12       5.727778              0.679451         8.106  \n",
      "24       5.727778              0.679451         8.106  \n",
      "36       5.727778              0.679451         8.106  \n",
      "48       5.727778              0.679451         8.106  \n",
      "498     2010-09-10\n",
      "691     2011-11-25\n",
      "2315    2010-02-12\n",
      "6735    2012-09-07\n",
      "6810    2010-12-31\n",
      "6815    2012-02-10\n",
      "6820    2011-09-09\n",
      "Name: date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate store/type combinations\n",
    "store_types = sales.drop_duplicates(subset=[\"store\", \"type\"])\n",
    "print(store_types.head())\n",
    "\n",
    "# Drop duplicate store/department combinations\n",
    "store_depts = sales.drop_duplicates(subset=[\"store\", \"department\"])\n",
    "print(store_depts.head())\n",
    "\n",
    "# Subset the rows where is_holiday is True and drop duplicate dates\n",
    "holiday_dates = sales[sales[\"is_holiday\"] == True].drop_duplicates(subset=\"date\")\n",
    "\n",
    "# Print date col of holiday_dates\n",
    "print(holiday_dates[\"date\"])\n",
    "\n",
    "# Count the number of stores of each type\n",
    "store_counts = store_types[\"type\"].value_counts()\n",
    "print(store_counts)\n",
    "\n",
    "# Get the proportion of stores of each type\n",
    "store_props = store_types[\"type\"].value_counts(normalize=True)\n",
    "print(store_props)\n",
    "\n",
    "# Count the number of each department number and sort\n",
    "dept_counts_sorted = store_depts[\"department\"].value_counts(sort=True)\n",
    "print(dept_counts_sorted)\n",
    "\n",
    "# Get the proportion of departments of each number and sort\n",
    "dept_props_sorted = store_depts[\"department\"].value_counts(sort=True, normalize=True)\n",
    "print(dept_props_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouped summary statistics\n",
    "There are 2 ways of groupingdata to calculate statistics: \n",
    "1. By subsetting the databse on the conditions we need. \n",
    "2. Using the `.groupby()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9097747 0.0902253 0.       ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sales = pd.read_csv(\"Datasets\\sales_subset.csv\")\n",
    "\n",
    "# Calc total weekly sales\n",
    "sales_all = sales[\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type A stores, calc total weekly sales\n",
    "sales_A = sales[sales[\"type\"] == \"A\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type B stores, calc total weekly sales\n",
    "sales_B = sales[sales[\"type\"] == \"B\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Subset for type C stores, calc total weekly sales\n",
    "sales_C = sales[sales[\"type\"] == \"C\"][\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = [sales_A, sales_B, sales_C] / sales_all\n",
    "print(sales_propn_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by type; calc total weekly sales\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Get proportion for each type\n",
    "sales_propn_by_type = sales_by_type / sum(sales[\"weekly_sales\"])\n",
    "print(sales_propn_by_type)\n",
    "\n",
    "# From previous step\n",
    "sales_by_type = sales.groupby(\"type\")[\"weekly_sales\"].sum()\n",
    "\n",
    "# Group by type and is_holiday; calc total weekly sales\n",
    "sales_by_type_is_holiday = sales.groupby([\"type\", \"is_holiday\"])[\"weekly_sales\"].sum()\n",
    "print(sales_by_type_is_holiday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         min        max          mean    median\n",
      "type                                           \n",
      "A    -1098.0  293966.05  23674.667242  11943.92\n",
      "B     -798.0  232558.51  25696.678370  13336.08\n",
      "     unemployment                         fuel_price_usd_per_l            \\\n",
      "              min    max      mean median                  min       max   \n",
      "type                                                                       \n",
      "A           3.879  8.992  7.972611  8.067             0.664129  1.107410   \n",
      "B           7.170  9.765  9.279323  9.199             0.760023  1.107674   \n",
      "\n",
      "                          \n",
      "          mean    median  \n",
      "type                      \n",
      "A     0.744619  0.735455  \n",
      "B     0.805858  0.803348  \n"
     ]
    }
   ],
   "source": [
    "# Import numpy with the alias np\n",
    "import numpy as np\n",
    "\n",
    "# For each store type, aggregate weekly_sales: get min, max, mean, and median\n",
    "sales_stats = sales.groupby(\"type\")[\"weekly_sales\"].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print sales_stats\n",
    "print(sales_stats)\n",
    "\n",
    "# For each store type, aggregate unemployment and fuel_price_usd_per_l: get min, max, mean, and median\n",
    "unemp_fuel_stats = sales.groupby(\"type\")[[\"unemployment\",\"fuel_price_usd_per_l\"]].agg([np.min, np.max, np.mean, np.median])\n",
    "\n",
    "# Print unemp_fuel_stats\n",
    "print(unemp_fuel_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot tables\n",
    "the `.pivot_table()` method is an alternative to `.groupby()`\n",
    "\n",
    "#### Structure of the .pivot_table() method \n",
    "`datframe.pivot_table(values='column_to_calculate', index='column_to_group_by_in_row', columns='column_to_group_by_columns', fill_value=0, margins=True)`\n",
    "\n",
    "**Parameters:**\n",
    "- values: columns in which we want to make the calculations\n",
    "- index: columns to group by as rows\n",
    "- columns: columns to group by as columns\n",
    "- aggfunc: the default calculation will be the mean. To calculate other statistic we need to specify it using the aggfunc parameter. \n",
    "- fill_value: by default when there is no data the pivot table will be filled with NaN. To change it we will use the fill_value method. It replaces missing values with a real value (known as imputation). \n",
    "- margins: is a shortcut for when you pivoted by two variables, but also wanted to pivot by each of those variables separately: it gives the row and column totals of the pivot table contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      weekly_sales\n",
      "type              \n",
      "A     23674.667242\n",
      "B     25696.678370\n",
      "           median          mean\n",
      "     weekly_sales  weekly_sales\n",
      "type                           \n",
      "A        11943.92  23674.667242\n",
      "B        13336.08  25696.678370\n",
      "is_holiday         False      True \n",
      "type                               \n",
      "A           23768.583523  590.04525\n",
      "B           25751.980533  810.70500\n",
      "type                    A              B\n",
      "department                              \n",
      "1            30961.725379   44050.626667\n",
      "2            67600.158788  112958.526667\n",
      "3            17160.002955   30580.655000\n",
      "4            44285.399091   51219.654167\n",
      "5            34821.011364   63236.875000\n",
      "...                   ...            ...\n",
      "95          123933.787121   77082.102500\n",
      "96           21367.042857    9528.538333\n",
      "97           28471.266970    5828.873333\n",
      "98           12875.423182     217.428333\n",
      "99             379.123659       0.000000\n",
      "\n",
      "[80 rows x 2 columns]\n",
      "type                   A              B           All\n",
      "department                                           \n",
      "1           30961.725379   44050.626667  32052.467153\n",
      "2           67600.158788  112958.526667  71380.022778\n",
      "3           17160.002955   30580.655000  18278.390625\n",
      "4           44285.399091   51219.654167  44863.253681\n",
      "5           34821.011364   63236.875000  37189.000000\n",
      "...                  ...            ...           ...\n",
      "96          21367.042857    9528.538333  20337.607681\n",
      "97          28471.266970    5828.873333  26584.400833\n",
      "98          12875.423182     217.428333  11820.590278\n",
      "99            379.123659       0.000000    379.123659\n",
      "All         23674.667242   25696.678370  23843.950149\n",
      "\n",
      "[81 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Pivot for mean weekly_sales for each store type\n",
    "mean_sales_by_type = sales.pivot_table(values='weekly_sales', index='type')\n",
    "\n",
    "# Print mean_sales_by_type\n",
    "print(mean_sales_by_type)\n",
    "\n",
    "# Import NumPy as np\n",
    "import numpy as np\n",
    "\n",
    "# Pivot for mean and median weekly_sales for each store type\n",
    "mean_med_sales_by_type = sales.pivot_table(values='weekly_sales', index='type', aggfunc=[np.median, np.mean])\n",
    "\n",
    "# Print mean_med_sales_by_type\n",
    "print(mean_med_sales_by_type)\n",
    "\n",
    "# Pivot for mean weekly_sales by store type and holiday \n",
    "mean_sales_by_type_holiday = sales.pivot_table(values='weekly_sales', index='type', columns='is_holiday')\n",
    "\n",
    "# Print mean_sales_by_type_holiday\n",
    "print(mean_sales_by_type_holiday)\n",
    "\n",
    "# Print mean weekly_sales by department and type; fill missing values with 0\n",
    "print(sales.pivot_table(values='weekly_sales', index='department', columns='type', fill_value=0))\n",
    "\n",
    "# Print the mean weekly_sales by department and type; fill missing values with 0s; sum all rows and cols\n",
    "print(sales.pivot_table(values=\"weekly_sales\", index=\"department\", columns=\"type\", fill_value=0,margins=True))\n",
    "\n",
    "    # The column 'All' returns an overall mean for each department, not (A+B)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing and Indexing Dataframes\n",
    "----\n",
    "### Explicit indexes \n",
    "- Setting a columns as index: `df.set_index(\"column_name\")`\n",
    "- Removing an index: `df.reset_index()`. Reset_index() has a parameter drop that will completely remove the index column when set to True (`df.reset_index(drep=True)`).\n",
    "\n",
    "**Why to use indexes?**\n",
    "It makes subsetting simpler. \n",
    "**Example:** \n",
    "- Without name as index: `df[df[\"name\"].isin([\"Stella\", \"Bella\"])]`\n",
    "- with name as index: `df.loc[[\"Stella\", \"Bella\"]]`\n",
    "\n",
    "#### Multi-level Indexes (Hierarchical Indexes)\n",
    "Several columns can be set as indexes at the same time by adding the columns in the set index method. \n",
    "**Example:** `df.set_index(\"column_1\", \"column_2\")`\n",
    "\n",
    "**Subsetting multi-level indexes**\n",
    "- To subset the outer level we will use a list: `df.loc[[\"labrador\", \"chihuahua\"]]`\n",
    "- To subset inner levels we will need a list of tuples: `df.loc[[(\"labrador\", \"brown\"), (\"chihuahua\", \"tan\")]]`\n",
    "\n",
    "#### Sorting by index values\n",
    "`df.sort_index()` will sort the values in ascending order by the index value from outer to inner. This order can be controlled: `df.sort_index(level=[\"column_1\", \"column_2\"], asceding=[True,False])`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Unnamed: 0        date     city        country  avg_temp_c\n",
      "0               0  2000-01-01  Abidjan  Côte D'Ivoire      27.293\n",
      "1               1  2000-02-01  Abidjan  Côte D'Ivoire      27.685\n",
      "2               2  2000-03-01  Abidjan  Côte D'Ivoire      29.061\n",
      "3               3  2000-04-01  Abidjan  Côte D'Ivoire      28.162\n",
      "4               4  2000-05-01  Abidjan  Côte D'Ivoire      27.547\n",
      "...           ...         ...      ...            ...         ...\n",
      "16495       16495  2013-05-01     Xian          China      18.979\n",
      "16496       16496  2013-06-01     Xian          China      23.522\n",
      "16497       16497  2013-07-01     Xian          China      25.251\n",
      "16498       16498  2013-08-01     Xian          China      24.528\n",
      "16499       16499  2013-09-01     Xian          China         NaN\n",
      "\n",
      "[16500 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "temperatures = pd.read_csv(\"Datasets/temperatures.csv\")\n",
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the index of temperatures to city\n",
    "temperatures_ind = temperatures.set_index(\"city\")\n",
    "\n",
    "# Look at temperatures_ind\n",
    "print(temperatures_ind)\n",
    "\n",
    "# Reset the temperatures_ind index, keeping its contents\n",
    "print(temperatures_ind.reset_index())\n",
    "\n",
    "# Reset the temperatures_ind index, dropping its contents\n",
    "print(temperatures_ind.reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a list of cities to subset on\n",
    "cities = [\"Moscow\", \"Saint Petersburg\"]\n",
    "\n",
    "# Subset temperatures using square brackets\n",
    "print(temperatures[temperatures[\"city\"].isin(cities)])\n",
    "\n",
    "# Subset temperatures_ind using .loc[]\n",
    "print(temperatures_ind.loc[cities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Unnamed: 0        date  avg_temp_c\n",
      "country  city                                              \n",
      "Brazil   Rio De Janeiro       12540  2000-01-01      25.974\n",
      "         Rio De Janeiro       12541  2000-02-01      26.699\n",
      "         Rio De Janeiro       12542  2000-03-01      26.270\n",
      "         Rio De Janeiro       12543  2000-04-01      25.750\n",
      "         Rio De Janeiro       12544  2000-05-01      24.356\n",
      "...                             ...         ...         ...\n",
      "Pakistan Lahore                8575  2013-05-01      33.457\n",
      "         Lahore                8576  2013-06-01      34.456\n",
      "         Lahore                8577  2013-07-01      33.279\n",
      "         Lahore                8578  2013-08-01      31.511\n",
      "         Lahore                8579  2013-09-01         NaN\n",
      "\n",
      "[330 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Index temperatures by country & city\n",
    "temperatures_ind = temperatures.set_index([\"country\", \"city\"])\n",
    "\n",
    "# List of tuples: Brazil, Rio De Janeiro & Pakistan, Lahore\n",
    "rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\", \"Lahore\")]\n",
    "\n",
    "# Subset for rows to keep\n",
    "print(temperatures_ind.loc[rows_to_keep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Unnamed: 0        date  avg_temp_c\n",
      "country     city                                      \n",
      "Afghanistan Kabul         7260  2000-01-01       3.326\n",
      "            Kabul         7261  2000-02-01       3.454\n",
      "            Kabul         7262  2000-03-01       9.612\n",
      "            Kabul         7263  2000-04-01      17.925\n",
      "            Kabul         7264  2000-05-01      24.658\n",
      "...                        ...         ...         ...\n",
      "Zimbabwe    Harare        5605  2013-05-01      18.298\n",
      "            Harare        5606  2013-06-01      17.020\n",
      "            Harare        5607  2013-07-01      16.299\n",
      "            Harare        5608  2013-08-01      19.232\n",
      "            Harare        5609  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n",
      "                       Unnamed: 0        date  avg_temp_c\n",
      "country       city                                       \n",
      "Côte D'Ivoire Abidjan           0  2000-01-01      27.293\n",
      "              Abidjan           1  2000-02-01      27.685\n",
      "              Abidjan           2  2000-03-01      29.061\n",
      "              Abidjan           3  2000-04-01      28.162\n",
      "              Abidjan           4  2000-05-01      27.547\n",
      "...                           ...         ...         ...\n",
      "China         Xian          16495  2013-05-01      18.979\n",
      "              Xian          16496  2013-06-01      23.522\n",
      "              Xian          16497  2013-07-01      25.251\n",
      "              Xian          16498  2013-08-01      24.528\n",
      "              Xian          16499  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n",
      "                    Unnamed: 0        date  avg_temp_c\n",
      "country     city                                      \n",
      "Afghanistan Kabul         7260  2000-01-01       3.326\n",
      "            Kabul         7261  2000-02-01       3.454\n",
      "            Kabul         7262  2000-03-01       9.612\n",
      "            Kabul         7263  2000-04-01      17.925\n",
      "            Kabul         7264  2000-05-01      24.658\n",
      "...                        ...         ...         ...\n",
      "Zimbabwe    Harare        5605  2013-05-01      18.298\n",
      "            Harare        5606  2013-06-01      17.020\n",
      "            Harare        5607  2013-07-01      16.299\n",
      "            Harare        5608  2013-08-01      19.232\n",
      "            Harare        5609  2013-09-01         NaN\n",
      "\n",
      "[16500 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Sort temperatures_ind by index values\n",
    "print(temperatures_ind.sort_index())\n",
    "\n",
    "# Sort temperatures_ind by index values at the city level\n",
    "print(temperatures_ind.sort_index(level=\"city\"))\n",
    "\n",
    "# Sort temperatures_ind by country then descending city\n",
    "print(temperatures_ind.sort_index(level=[\"country\",\"city\"], ascending=[True,False]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slicing and subsetting with .loc and .iloc\n",
    "To slice a database it is necesarry that we sort the values index first: `df.set_index([\"column_1\", \"column_2\"]).sort_index()`. \n",
    "\n",
    "#### Slice the outer index level\n",
    "To slice the outer index level we will use `.loc[]`. Example: `df.loc[\"Chow Chow\":\"Poodle\"]` (the final value is included).\n",
    "\n",
    "#### Slice the inner index levels\n",
    "To slice the inner index levels we will use the `.loc[]` fuction as well. However, the first ans last positions will be passed as tuples. Example: `df.loc[(\"Labrador\",\"Brown\"):(\"Schnauzer\", \"Grey\")]`\n",
    "\n",
    "#### Slicing columns\n",
    "The syntaxis is the same `df.loc[rows, columns]`.  \n",
    "- To get all rows and some columns: `df.loc[:, \"column_start\":\"column_end\"]`.\n",
    "- To slice on rows and columns at the same time: `df.loc[(\"Labrador\",\"Brown\"):(\"Schnauzer\", \"Grey\"), \"name\":\"height_cm\"]`.\n",
    "\n",
    "#### Subsetting by row / column number\n",
    "To do this we will use the `.iloc[rows, column]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the index of temperatures_ind\n",
    "temperatures_srt = temperatures_ind.sort_index()\n",
    "\n",
    "# Subset rows from Pakistan to Russia\n",
    "print(temperatures_srt.loc[\"Pakistan\": \"Russia\"])\n",
    "\n",
    "# Subset rows from Pakistan, Lahore to Russia, Moscow\n",
    "print(temperatures_srt.loc[(\"Pakistan\",\"Lahore\"): (\"Russia\", \"Moscow\")])\n",
    "\n",
    "# Subset rows from India, Hyderabad to Iraq, Baghdad\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\")])\n",
    "\n",
    "# Subset columns from date to avg_temp_c\n",
    "print(temperatures_srt.loc[:, \"date\":\"avg_temp_c\"])\n",
    "\n",
    "# Subset in both directions at once\n",
    "print(temperatures_srt.loc[(\"India\", \"Hyderabad\"):(\"Iraq\", \"Baghdad\"), \"date\":\"avg_temp_c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\n",
    "temperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-31\")]\n",
    "print(temperatures_bool)\n",
    "\n",
    "# Set date as the index and sort the index\n",
    "temperatures_ind = temperatures.set_index(\"date\").sort_index()\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\n",
    "print(temperatures_ind.loc[\"2010\":\"2011\"])\n",
    "\n",
    "# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\n",
    "print(temperatures_ind.loc[\"2010-08\":\"2011-02\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 23rd row, 2nd column (index 22, 1)\n",
    "print(temperatures.iloc[22,1])\n",
    "\n",
    "# Use slicing to get the first 5 rows\n",
    "print(temperatures.iloc[:5])\n",
    "\n",
    "# Use slicing to get columns 3 to 4\n",
    "print(temperatures.iloc[:, 2:4])\n",
    "\n",
    "# Use slicing in both directions at once\n",
    "print(temperatures.iloc[:5,2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with pivot tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a year column to temperatures\n",
    "temperatures[\"year\"] = temperatures[\"date\"].dt.year\n",
    "\n",
    "# Pivot avg_temp_c by country and city vs year\n",
    "temp_by_country_city_vs_year = temperatures.pivot_table(values=\"avg_temp_c\", index=[\"country\", \"city\"], columns=\"year\")\n",
    "\n",
    "# See the result\n",
    "print(temp_by_country_city_vs_year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset for Egypt to India\n",
    "temp_by_country_city_vs_year.loc[\"Egypt\":\"India\"]\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi\n",
    "temp_by_country_city_vs_year.loc[(\"Egypt\",\"Cairo\"):(\"India\", \"Delhi\")]\n",
    "\n",
    "# Subset for Egypt, Cairo to India, Delhi, and 2005 to 2010\n",
    "temp_by_country_city_vs_year.loc[(\"Egypt\",\"Cairo\"):(\"India\", \"Delhi\"), \"2005\":\"2010\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the worldwide mean temp by year\n",
    "mean_temp_by_year = temp_by_country_city_vs_year.mean()\n",
    "\n",
    "# Filter for the year that had the highest mean temp\n",
    "print(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n",
    "\n",
    "# Get the mean temp by city\n",
    "mean_temp_by_city = temp_by_country_city_vs_year.mean(axis=\"columns\")\n",
    "\n",
    "# Filter for the city that had the lowest mean temp\n",
    "print(mean_temp_by_city[mean_temp_by_city == mean_temp_by_city.min()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating and Visualizing DataFrames\n",
    "-----\n",
    "### Visualizing your data\n",
    "To be able to diplay our visualizations we will need to import pyplot from matplotlib: \n",
    "\n",
    "`import matplotlib.pyplot as plt` \n",
    "\n",
    "#### Types of plots\n",
    "| Histograms | Bar plot | Line plot | Scatter plot |\n",
    "| ---------- | -------- | ---------- | ---------- | \n",
    "|  | Reveal relationships between a categorical variable and a numeric variable | Line plots are great for visualizing changes in numeric variables over time | Scatter plots are great for visualizing relationships between two numeric variables | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date          type  year  avg_price   size     nb_sold\n",
      "0  2015-12-27  conventional  2015       0.95  small  9626901.09\n",
      "1  2015-12-20  conventional  2015       0.98  small  8710021.76\n",
      "2  2015-12-13  conventional  2015       0.93  small  9855053.66\n",
      "3  2015-12-06  conventional  2015       0.89  small  9405464.36\n",
      "4  2015-11-29  conventional  2015       0.99  small  8094803.56\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "avocados = pd.read_pickle(\"Datasets/avoplotto.pkl\")\n",
    "\n",
    "# Look at the first few rows of data\n",
    "print(avocados.head())\n",
    "\n",
    "# Get the total number of avocados sold of each size\n",
    "nb_sold_by_size = avocados.groupby(\"size\")[\"nb_sold\"].sum()\n",
    "\n",
    "# Create a bar plot of the number of avocados sold by size\n",
    "nb_sold_by_size.plot(kind=\"bar\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the total number of avocados sold on each date\n",
    "nb_sold_by_date = avocados.groupby(\"date\")[\"nb_sold\"].sum()\n",
    "\n",
    "# Create a line plot of the number of avocados sold by date\n",
    "nb_sold_by_date.plot(x=\"date\", y=\"nb_sold\", kind='line', rot=45)\n",
    "    # rot will rotate the lables in X axis\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot of avg_price vs. nb_sold with title\n",
    "avocados.plot(x=\"nb_sold\", y=\"avg_price\", kind='scatter', title=\"Number of avocados sold vs. average price\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of conventional avg_price \n",
    "avocados[avocados[\"type\"]==\"conventional\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "# Histogram of organic avg_price\n",
    "avocados[avocados[\"type\"]==\"organic\"][\"avg_price\"].hist(alpha=0.5, bins=20)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend([\"conventional\", \"organic\"])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "in a pandas Dataframe, the missing values are indicated by NaN (Not a Number).  \n",
    "- To idenfify the missing values we can use the isna() method as follows: `df.isna()`. The result will be a dataframe with booleans identifing value = NaN. \n",
    "- For a better view of the missing values we can add the .any() method `df.isna().any()`. The result will list of booleans that identify columns that have at least one NaN \n",
    "- `df.isna().sum()` will provide the count of missing values.\n",
    "\n",
    "#### What can we do with missing values? \n",
    "\n",
    "1. **Removing the missing values**\n",
    "To remove the missing values we can us `df.dropna()`. This will delete all the rows that have NaN values. \n",
    "\n",
    "2. **Replacing the missing values**\n",
    "We can replace the missing values with a value using `df.fillna(value)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      date   type   year  avg_price   size  nb_sold\n",
      "52   False  False  False      False  False    False\n",
      "53   False  False  False      False  False    False\n",
      "54   False  False  False      False  False    False\n",
      "55   False  False  False      False  False    False\n",
      "56   False  False  False      False  False    False\n",
      "..     ...    ...    ...        ...    ...      ...\n",
      "944  False  False  False      False  False    False\n",
      "945  False  False  False      False  False    False\n",
      "946  False  False  False      False  False    False\n",
      "947  False  False  False      False  False    False\n",
      "948  False  False  False      False  False    False\n",
      "\n",
      "[312 rows x 6 columns]\n",
      "date         False\n",
      "type         False\n",
      "year         False\n",
      "avg_price    False\n",
      "size         False\n",
      "nb_sold      False\n",
      "dtype: bool\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import matplotlib.pyplot with alias plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "avocados_2016 = avocados[(avocados[\"date\"] > \"2016-01-01\") & (avocados[\"date\"] < \"2017-01-01\")]\n",
    "avocados_2016\n",
    "\n",
    "# Check individual values for missing values\n",
    "print(avocados_2016.isna())\n",
    "\n",
    "# Check each column for missing values\n",
    "print(avocados_2016.isna().any())\n",
    "\n",
    "# Bar plot of missing values by variable\n",
    "avocados_2016.isna().sum().plot(kind=\"bar\")\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing values\n",
    "avocados_complete = avocados_2016.dropna()\n",
    "\n",
    "# Check if any columns contain missing values\n",
    "print(avocados_complete.isna().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous step\n",
    "cols_with_missing = [\"small_sold\", \"large_sold\", \"xl_sold\"]\n",
    "avocados_2016[cols_with_missing].hist()\n",
    "plt.show()\n",
    "\n",
    "# Fill in missing values with 0\n",
    "avocados_filled = avocados_2016.fillna(0)\n",
    "\n",
    "# Create histograms of the filled columns\n",
    "avocados_filled[cols_with_missing].hist()\n",
    "plt.show()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrames\n",
    "\n",
    "Dictionaries:\n",
    "\n",
    "```python\n",
    "my_dict = {\n",
    "    \"key1\": value1,\n",
    "    \"key2\": value2,\n",
    "    \"key3\": value3\n",
    "}\n",
    "```\n",
    "To access values in a dictionary, use [] `my_dict[\"key1\"]`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries with new data\n",
    "avocados_list = [\n",
    "    {\"date\": \"2019-11-03\", \"small_sold\": 10376832, \"large_sold\": 7835071},\n",
    "    {\"date\": \"2019-11-10\", \"small_sold\": 10717154, \"large_sold\": 8561348},\n",
    "]\n",
    "\n",
    "# Convert list into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_list)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of lists with new data\n",
    "avocados_dict = {\n",
    "    \"date\": [\"2019-11-17\", \"2019-12-01\"\t],\n",
    "    \"small_sold\": [10859987, 9291631],\n",
    "    \"large_sold\": [7674135, 6238096]\n",
    "}\n",
    "\n",
    "# Convert dictionary into DataFrame\n",
    "avocados_2019 = pd.DataFrame(avocados_dict)\n",
    "\n",
    "# Print the new DataFrame\n",
    "print(avocados_2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and Writing CSV\n",
    "CSV (Comma-separated values) is a common data storage type designed to store tabular data. \n",
    "- To import data from a csv we will use `df.read_csv(\"file_path/file.csv\")`\n",
    "- To convert a DataFramse into a CSV we will use `df.to_csv(\"new_file_name.csv\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
